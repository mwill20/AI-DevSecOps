// ğŸ›¡ï¸ IMPROVED ADVERSARIAL TEST: Enhanced Security Validator Testing
// This script tests with improved pattern matching to catch all vulnerabilities

import fs from 'fs';

console.log('ğŸ›¡ï¸ IMPROVED ADVERSARIAL TESTING: Security Standards Validator');
console.log('=' .repeat(70));
console.log('ğŸ”´ RED TEAM EXERCISE: Enhanced vulnerability detection\n');

// Enhanced vulnerability patterns
const vulnerabilityPatterns = {
    // LLM06: Hardcoded secrets - IMPROVED PATTERNS
    secrets: [
        // OpenAI API keys
        /sk-[a-zA-Z0-9]{48}/g,
        // AWS Access Keys
        /AKIA[a-zA-Z0-9]{16}/g,
        // Google API keys
        /AIza[a-zA-Z0-9_-]{35}/g,
        // GitHub tokens
        /ghp_[a-zA-Z0-9]{36}/g,
        // Generic patterns
        /(?:api_key|apikey|secret|token|password|pwd)\s*=\s*["']([a-zA-Z0-9_\-!@#$%^&*]{10,})["']/gi,
        // Database passwords
        /["']([a-zA-Z0-9_\-!@#$%^&*]{8,})["'].*(?:password|pwd)/gi,
        // Any long alphanumeric string that looks like a key
        /["']([a-zA-Z0-9_-]{20,})["']/g
    ],
    
    // LLM01: Prompt injection patterns - IMPROVED
    promptInjection: [
        /(?:prompt|input)\s*=\s*f["'`][^"'`]*?(?:ignore|forget|disregard|system|admin|root)/gi,
        /f["'`][^"'`]*?[^"'`]*?(?:ignore|forget|disregard|system|admin|root)/gi,
        /user.*input.*prompt/gi,
        /system_prompt.*user_input/gi,
        /concat.*user.*input/gi
    ],
    
    // LLM02: Insecure output handling
    insecureOutput: [
        /innerHTML\s*=/gi,
        /dangerouslySetInnerHTML/gi,
        /outerHTML\s*=/gi,
        /\.html\s*\(/gi
    ],
    
    // Agent OS standards
    codingStandards: [
        /console\.(log|debug|info|warn|error)\s*\([^)]*["'`][^"'`]*?(?:password|secret|token|key)/gi,
        /catch\s*\(\s*\)\s*\{\s*\}/gi,
        /except\s*:/gi
    ]
};

const testFiles = [
    'test_vulnerability.py',
    'test_vulnerability.js', 
    'test_vulnerability.tsx'
];

let totalSecrets = 0;
let totalPromptInjections = 0;
let totalInsecureOutput = 0;
let totalStandardsViolations = 0;
const allDetectedViolations = [];

testFiles.forEach(file => {
    try {
        const content = fs.readFileSync(file, 'utf8');
        console.log(`\nğŸ“ ${file}:`);
        
        // Test for secrets (LLM06) - Enhanced detection
        const secrets = [];
        vulnerabilityPatterns.secrets.forEach(pattern => {
            const matches = content.match(pattern);
            if (matches) {
                // Remove duplicates
                matches.forEach(match => {
                    if (!secrets.includes(match)) {
                        secrets.push(match);
                    }
                });
            }
        });
        
        if (secrets.length > 0) {
            console.log(`   ğŸš¨ LLM06 - Hardcoded Secrets: ${secrets.length}`);
            secrets.forEach((secret, i) => {
                console.log(`     ${i+1}. ${secret.substring(0, 60)}...`);
                allDetectedViolations.push({ file, type: 'LLM06', severity: 'CRITICAL', content: secret });
            });
            totalSecrets += secrets.length;
        }
        
        // Test for prompt injection (LLM01) - Enhanced detection
        const promptInjections = [];
        vulnerabilityPatterns.promptInjection.forEach(pattern => {
            const matches = content.match(pattern);
            if (matches) {
                matches.forEach(match => {
                    if (!promptInjections.includes(match)) {
                        promptInjections.push(match);
                    }
                });
            }
        });
        
        if (promptInjections.length > 0) {
            console.log(`   ğŸš¨ LLM01 - Prompt Injection: ${promptInjections.length}`);
            promptInjections.forEach((injection, i) => {
                console.log(`     ${i+1}. ${injection.substring(0, 60)}...`);
                allDetectedViolations.push({ file, type: 'LLM01', severity: 'HIGH', content: injection });
            });
            totalPromptInjections += promptInjections.length;
        }
        
        // Test for insecure output (LLM02)
        const insecureOutputs = [];
        vulnerabilityPatterns.insecureOutput.forEach(pattern => {
            const matches = content.match(pattern);
            if (matches) {
                matches.forEach(match => {
                    if (!insecureOutputs.includes(match)) {
                        insecureOutputs.push(match);
                    }
                });
            }
        });
        
        if (insecureOutputs.length > 0) {
            console.log(`   ğŸš¨ LLM02 - Insecure Output: ${insecureOutputs.length}`);
            insecureOutputs.forEach((output, i) => {
                console.log(`     ${i+1}. ${output.substring(0, 60)}...`);
                allDetectedViolations.push({ file, type: 'LLM02', severity: 'MEDIUM', content: output });
            });
            totalInsecureOutput += insecureOutputs.length;
        }
        
        // Test for coding standards violations
        const standardsViolations = [];
        vulnerabilityPatterns.codingStandards.forEach(pattern => {
            const matches = content.match(pattern);
            if (matches) {
                matches.forEach(match => {
                    if (!standardsViolations.includes(match)) {
                        standardsViolations.push(match);
                    }
                });
            }
        });
        
        if (standardsViolations.length > 0) {
            console.log(`   ğŸš¨ Standards Violations: ${standardsViolations.length}`);
            standardsViolations.forEach((violation, i) => {
                console.log(`     ${i+1}. ${violation.substring(0, 60)}...`);
                allDetectedViolations.push({ file, type: 'CODING_STANDARDS', severity: 'LOW', content: violation });
            });
            totalStandardsViolations += standardsViolations.length;
        }
        
        if (secrets.length === 0 && promptInjections.length === 0 && 
            insecureOutputs.length === 0 && standardsViolations.length === 0) {
            console.log(`   âœ… No vulnerabilities detected`);
        }
        
    } catch (error) {
        console.log(`âŒ Failed to test ${file}:`, error.message);
    }
});

console.log('\nğŸ“Š ENHANCED RESULTS:');
console.log('-'.repeat(50));
console.log(`LLM06 - Hardcoded Secrets: ${totalSecrets} violations`);
console.log(`LLM01 - Prompt Injection: ${totalPromptInjections} violations`);
console.log(`LLM02 - Insecure Output: ${totalInsecureOutput} violations`);
console.log(`Agent OS Standards: ${totalStandardsViolations} violations`);
console.log(`Total Violations Detected: ${allDetectedViolations.length}`);

// Show all detected violations in detail
console.log('\nğŸ” DETAILED VIOLATION ANALYSIS:');
console.log('-'.repeat(50));
allDetectedViolations.forEach((violation, index) => {
    console.log(`${index + 1}. [${violation.severity}] ${violation.type} in ${violation.file}`);
    console.log(`   Content: ${violation.content}`);
    console.log('');
});

// Test 2: Hard Guardrail Simulation
console.log('ğŸ” TEST 2: Hard Guardrail - Phase Transition Blocking');
console.log('-'.repeat(50));

const criticalViolations = totalSecrets; // Secrets are critical
const highViolations = totalPromptInjections; // Prompt injection is high
const totalViolations = criticalViolations + highViolations + totalInsecureOutput + totalStandardsViolations;

console.log(`ğŸš€ Attempting phase transition: WINDSURF â†’ ANTI_GRAVITY`);
console.log(`   Total Violations: ${totalViolations}`);
console.log(`   Critical Violations: ${criticalViolations}`);
console.log(`   High Violations: ${highViolations}`);

// Simulate the guardrail logic
const guardrailShouldBlock = criticalViolations > 0 || highViolations > 0;
const proceedButtonDisabled = criticalViolations > 0;

console.log(`\nğŸ” GUARDRAIL DECISION:`);
console.log(`   Block Transition: ${guardrailShouldBlock ? 'YES âœ…' : 'NO âŒ'}`);
console.log(`   Proceed Button Disabled: ${proceedButtonDisabled ? 'YES âœ…' : 'NO âŒ'}`);
console.log(`   Override Required: ${guardrailShouldBlock ? 'YES âœ…' : 'NO âŒ'}`);

if (guardrailShouldBlock) {
    console.log(`\nğŸš¨ TERMINAL MODAL STATE:`);
    console.log(`   Modal: OPEN and BLOCKING`);
    console.log(`   Proceed Button: GRAYED OUT`);
    console.log(`   Fix Button: ENABLED`);
    console.log(`   Override Button: ENABLED (with justification)`);
    console.log(`   User Action: FORCED to fix or justify override`);
    
    console.log(`\nğŸ“‹ VIOLATIONS BLOCKING TRANSITION:`);
    allDetectedViolations
        .filter(v => v.severity === 'CRITICAL' || v.severity === 'HIGH')
        .forEach((violation, i) => {
            console.log(`${i+1}. [${violation.severity}] ${violation.type}: ${violation.content.substring(0, 50)}...`);
        });
}

// Test 3: False Positive Check
console.log('\nğŸ” TEST 3: False Positive Check');
console.log('-'.repeat(50));

const cleanCode = `
// This is clean code - should not trigger any violations
const API_KEY = process.env.API_KEY;
const userInput = sanitizeInput(userInput);
const prompt = \`Tell me about \${userInput}\`;

try {
    const result = await apiCall();
    console.log('Success');
} catch (error) {
    logger.error('Operation failed', { error });
    throw new Error('Could not complete operation');
}
`;

fs.writeFileSync('test_clean_code.js', cleanCode);

const cleanFileContent = fs.readFileSync('test_clean_code.js', 'utf8');
let falsePositives = 0;

vulnerabilityPatterns.secrets.forEach(pattern => {
    if (pattern.test(cleanFileContent)) {
        falsePositives++;
    }
});

vulnerabilityPatterns.promptInjection.forEach(pattern => {
    if (pattern.test(cleanFileContent)) {
        falsePositives++;
    }
});

vulnerabilityPatterns.insecureOutput.forEach(pattern => {
    if (pattern.test(cleanFileContent)) {
        falsePositives++;
    }
});

vulnerabilityPatterns.codingStandards.forEach(pattern => {
    if (pattern.test(cleanFileContent)) {
        falsePositives++;
    }
});

console.log(`Clean Code Test: ${falsePositives === 0 ? 'âœ… PASS' : 'âŒ FAIL'}`);
console.log(`False Positives: ${falsePositives}`);

// Clean up test file
fs.unlinkSync('test_clean_code.js');

// Final Assessment
console.log('\nğŸ“‹ ENHANCED ADVERSARIAL TESTING REPORT');
console.log('=' .repeat(70));

const tests = [
    { name: 'LLM06 - Hardcoded Secrets', passed: totalSecrets >= 6, critical: true },
    { name: 'LLM01 - Prompt Injection', passed: totalPromptInjections >= 4, critical: true },
    { name: 'LLM02 - Insecure Output', passed: totalInsecureOutput >= 2, critical: false },
    { name: 'Agent OS Standards', passed: totalStandardsViolations >= 3, critical: false },
    { name: 'Hard Guardrail Blocking', passed: guardrailShouldBlock, critical: true },
    { name: 'False Positive Check', passed: falsePositives === 0, critical: false }
];

const passedTests = tests.filter(t => t.passed).length;
const criticalTestsPassed = tests.filter(t => t.critical && t.passed).length;

console.log(`\nğŸ“Š SUMMARY:`);
console.log(`   Total Tests: ${tests.length}`);
console.log(`   Passed: ${passedTests}/${tests.length}`);
console.log(`   Critical Tests Passed: ${criticalTestsPassed}/${tests.filter(t => t.critical).length}`);
console.log(`   Success Rate: ${Math.round((passedTests / tests.length) * 100)}%`);

console.log(`\nğŸ“‹ DETAILED RESULTS:`);
tests.forEach((test, index) => {
    console.log(`${index + 1}. ${test.name}: ${test.passed ? 'âœ… PASS' : 'âŒ FAIL'} ${test.critical ? '(CRITICAL)' : ''}`);
});

const readyForIntegration = criticalTestsPassed === tests.filter(t => t.critical).length && falsePositives === 0;

console.log(`\nğŸ¯ SECURITY ASSESSMENT:`);
console.log(`   Critical Vulnerabilities Detected: ${totalSecrets + totalPromptInjections} âœ…`);
console.log(`   Guardrail Blocking: ${guardrailShouldBlock ? 'WORKING âœ…' : 'FAILED âŒ'}`);
console.log(`   False Positives: ${falsePositives === 0 ? 'NONE âœ…' : 'DETECTED âŒ'}`);

console.log(`\nğŸš€ INTEGRATION READINESS:`);
console.log(`   Status: ${readyForIntegration ? 'âœ… READY FOR INTEGRATION' : 'âŒ NOT READY'}`);

if (readyForIntegration) {
    console.log(`\nâœ… ADVERSARIAL TESTING PASSED!`);
    console.log(`âœ… Security Standards Validator is working correctly!`);
    console.log(`âœ… Hard guardrails block bad code as expected!`);
    console.log(`âœ… Ready for integration into Windsurf, Anti-Gravity, and VS Code!`);
} else {
    console.log(`\nâŒ ADVERSARIAL TESTING FAILED!`);
    console.log(`âŒ Security guardrails need fixes before integration!`);
}

console.log('\n' + '=' .repeat(70));
